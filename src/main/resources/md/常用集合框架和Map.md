# 常用集合框架和Map

1. ## HashMap

   1. ### 特点

      > - jdk1.7中使用头插法，并发扩容的情况下会产生死链问题，jdk1.8中使用尾插法
      > - jdk1.8之前是数组+链表结构，jdk1.8及之后是数组+链表/红黑树结构
      > - 当数组的大小大于64且链表的长度大于8时链表才会转换位红黑树，当数节点小于6时会退化为链表
      > - 相比HashTable来说可以存放null值，且默认存放在index为0的位置

   2. ### 问题

      - #### 为什么需要数组大小超过64时才会进行红黑树转换？

      - #### 为什么用红黑树而不是B树、B+树？

      - #### 为什么节点数超过8时会转换为红黑树但是需要节点数少于6时才退化为链表？

        > 给一个缓冲的空间，避免频繁的红黑树和链表之间的相互转换。JDK官方在HashMap line 175给出的注释说TreeNode的内存占用是Node的两倍，而且基于泊松分布来看链表长度为8的概率已经很小了，所以出于空间和时间的均衡来考虑需要超过8的大小才会转化为红黑树。另外红黑树的平均时间复杂度是O(logn)，链表的平均时间复杂度是O(n/2)，当n == 9时log约等于3，而n/2 == 4.5，此时开始红黑树查询效率要更高，但是当n == 5时logn约等于2.6，而n/2 = 2.5，此时开始链表的查询效率更优，所以使用6和8来作为链表和红黑树的转化阈值。
        >
      
        ------
      
        
      
      - #### 什么是死链？
      
        jdk1.7中对HashMap进行并发扩容时会导致链表变成循环链表导致死链。
      
        ------
      
        
      
      - #### 如何计算节点所处下标索引？
      
        (capacity-1)&(hash^(hash>>>16))
      
        ------
      
        
      
      - #### 为什么要进行hash^(hash>>>16)这样的操作
      
        > 这样做优化了高16位的算法，保证在数组长度比较小的的时候高16位也可以参与到hash运算中来，同时也不会有太大的性能开销
        >

        ------

        

      - #### 还有什么其他的计算下标索引的方式吗？

        取余数法、平方取中法、伪随机数法

        ------

        

      - #### 为什么不用取余数法？

        > 因为计算机底层使用位运算很快，但是取余运算是通过一个个的减法求值的，很慢，所以用位运算来代替取余运算。并且HashMap的数组长度总是为2的n次幂，所以hashcode%capacity这种运算就等价于hashcode&(capacity-1)，这也时出于效率方面的考虑才这样设计的。
        >
      
        ------
      
        
      
      - #### 什么是哈希碰撞？发生哈希碰撞之后的存储过程时怎样的？
      
        > 哈希碰撞就是指两个对象计算出来的hash值是一样的，这种情况下需要判断对象的equals方法是否相等，相等的话会覆盖之前元素的值，不相等的话会跟链表的下一个节点进行比较，依此类推，如果一直不相等则会重新新创建一个元素进行添加。
        >

        ------

        

      - #### 为什么负载因子默认设置为0.75f？能不能设置为其它值？甚至大于1

        > 默认值0.75是出于时间和空间的均衡考虑而设置的，threshold = loadFactor * capacity表示此时Map中可以容纳的KV-Entry的最大个数，因此loadFactor决定了相同数组长度下可以容纳的KV-Entry的个数，如果在空间充足，但是对时间要求很高的情况下可以适当调小loadFactor的值，这样可以减少hash碰撞的发生增加查询效率，在空间不足，但是对查询效率没有要求的时候可以增大LoadFactor的值。loadFactor的值可以大于1。
        >
      
        ------
      
        
      
      - #### 什么时候进行扩容？以及扩容的过程
      
        > 当数组上存放超过capacity*0.75f个元素，或者是Map中存放的Node少于64但是数组中某个链表的长度已经超过了8时会进行扩容
        >

        ------

        

      - #### 默认初始化大小是多少？最大容量是多少？如果手动指定了initcapacity为10，那么最终的capacity是多少？

        > 默认初始化大小为16。最大容量是2的30次幂。最终capacity为16。因为为了优化%运算，所以capacity必须是2的n次幂，因此会寻找第一个大于等于10同时又是2的n次幂的数作为capacity。
        >
      
        ------
      
        
      
      - #### 如果你一开始确定有13个元素将要存到Map中，那么你会怎么设置Map的初始化大小？
      
        ( ( float ) 13 / 0.75f ) + 1.0f，最后+1.0f的原因是为了避免之后再添加元素时直接发生扩容，这样做相当于是节省了一次扩容的时间。
      
        ------
      
        
      
      - #### HashMap的数组长度为什么总是为2的n次幂？
      
        一是为了通过使用位运算来取代%运算，二是因为在jdk1.7中扩容以后需要重新根据hash值计算index的位置，而jdk1.8中优化了这一块。
      
        ```
        扩容前
            oldLength-1 = 15			0000 1111
            hash = 16				&	0001 0000 = 0000 1111
            hash = 46				&	0010 1110 = 0000 1110
        扩容后
        	newLength-1 = 31			0001 1111
            hash = 16				&	0001 0000 = 0001 0000 
            hash = 46				&	0010 1110 = 0000 1110
        结果发现扩容后的index位置只和扩容后newLength-1的最高位的1有关，而扩容后newLength-1的最高位的1和扩容前oldLength最高1是相等的，所以可以直接得出扩容前index位置的Node在扩容后要不还是在index位置，要不就是在index+oldLength位置
        ```
      
        ------
      
        
      
      - #### 为什么HashMap可以存放null值而HashTable不可以？
      
        > 因为HashMap时fail-fast机制的，内部会维护一个modifiedCount值，如果遍历的时候这个值发生了变化，就会直接抛出异常。HashTable是fail-safe机制的，也就是说允许读取到不准确的值，如果允许null的存在，那就没法判断对应的key是不存在还是为null，因为没法调用contains方法来判断是否存在，ConcurrentHashMap也是一样。

2. ## ConcurrentHashMap

   - 特点
     - 不允许存放null的key和null的value